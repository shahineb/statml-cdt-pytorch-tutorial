{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz Time: Implement your own Deep Kernel Learning\n",
    "\n",
    "reference reading: https://arxiv.org/pdf/1511.02222.pdf\n",
    "\n",
    "### What is the biggest difference between kernel methods and neural networks? \n",
    "\n",
    "- Neural networks (NN) uses finitely many highly *adaptive* basis functions\n",
    "\n",
    "- while kernel methods (KM) essentially uses infinitely many *fixed* basis functions.\n",
    "\n",
    "In fact, one reason why NN works particularly well in practice is because is can automatically discover *meaningful representations* in high-dimensional data. \n",
    "\n",
    "On the other hand, kernel methods or Gaussian processes contains less design choice and are more straight forward to work with in practice.\n",
    "\n",
    "### What's really the difference?\n",
    "\n",
    "In NN, the key is about learning a representation of your data. For example,\n",
    "- Convolutional NN for capturing spatial characteristics within your image, \n",
    "- LSTM structure for recurrent structural data.\n",
    "\n",
    "While in kernel methods, we care more about how to utilise these high (infinite) dimensional representations and the rich theory behind functional spaces to run probabilistic inference on data. For example,\n",
    "- HSIC for testing statistical independence\n",
    "- Kernel Mean Emebdding for learning representations of distributions in a function space (the RKHS).\n",
    "- Causal discovery using Kernel Conditional Deviance for Causal Inference (KCDC).\n",
    "- Meta Learning (Ton et al.)\n",
    "\n",
    "### Which is better? \n",
    "\n",
    "To be honest, it really depends on how you use them. So instead of asking which is better, we should be asking how could one combine the advantages of each approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Other helper packages\n",
    "\n",
    "from gpytorch import lazify\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Boston data\n",
    "\n",
    "We will call in the sckit-learn inbuilt Boston housing dataset function for illustration. To know more about the API, see https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(load_boston()[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# Train-test split. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=41 # Fixed for reproducibility\n",
    "                                                   )\n",
    "\n",
    "# Tensorise your data. Watch out for dimensionality issues.\n",
    "\n",
    "train_x = torch.tensor(X_train).float()\n",
    "train_y = torch.tensor(y_train).float()\n",
    "\n",
    "test_x = torch.tensor(X_test).float()\n",
    "test_y = torch.tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick tutorial on Kernel methods\n",
    "\n",
    "\n",
    "## Kernel \n",
    "\n",
    "A kernel function $k : \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ is an inner product between two latent representations of $\\mathcal{X}$ in some Hilbert space $\\mathcal{H}$. In general, we can write $k(x, x')$ as,\n",
    "\n",
    "$$k(x, x') = \\langle \\phi(x), \\phi(x') \\rangle_{\\mathcal{H}} $$ \n",
    "\n",
    "We denote $\\phi: \\mathcal{X} \\rightarrow \\mathcal{H}$ as the feature map of a kernel functions. In practice, the most commonly used kernel is Radial-basis function kernel (besides the linear kernel), \n",
    "\n",
    "\\begin{align}\n",
    "k_{rbf}(x, x') = \\exp\\Big(\\frac{-||x - x'||_2^2}{l^2} \\Big)\n",
    "\\end{align}\n",
    "\n",
    "has an infinite dimensional feature map $\\phi_{rbf}$.\n",
    "\n",
    "Intuitively, a kernel function first maps your data into some infinite dimensional space and take inner product there to compute the similarity. \n",
    "\n",
    "\n",
    "### Deep feature maps\n",
    "\n",
    "Now we want you to implement your own feature map function using neural networks, aka we want a new $\\phi_d$ function that takes $\\mathcal{X}$ to $\\mathcal{G}$ where the dimensionality of $\\mathcal{G}$ depends on your final output layer in the NN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_map(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implement your own feature map function\n",
    "    \n",
    "    - Set up 2 hidden layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, h_layer1, h_layer2, output_dim):\n",
    "\n",
    "        super().__init__()\n",
    "        # set up the feature map structure\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snap an RBF kernel on top of these feature maps\n",
    "\n",
    "As Wilson et al. proposed, the simplest form of a Deep kernel is to put an rbf kernel on top of these feature maps,\n",
    "\\begin{align}\n",
    "k_{DeepRBF}(x, x') = k_{rbf}(\\phi_d(x), \\phi_d(x'))\n",
    "\\end{align}\n",
    "\n",
    "We have written a sample code for the RBF kernel below, can you incorporate it with your feature map function and come up with your DeepRBFKernel function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFkernel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    An RBF kernel for illustration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Parameterise the lengthscale so you can learn it later\n",
    "        \n",
    "        self.lengthscale = torch.nn.Parameter(torch.tensor(1.).float())\n",
    "        \n",
    "    def forward(self, x1, x2=None):\n",
    "        \"\"\"\n",
    "        if x2 is None, then k(x1) = k(x1, x1) returns the square kernel matrix\n",
    "        otherwise k(x1, x2) returns a x1_dim \\times x2_dim rectangular matrix\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if x2 is None:\n",
    "            x2 = x1\n",
    "        \n",
    "        return lazify(torch.exp(-torch.cdist(x1, x2, p=2)/self.lengthscale))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side tutorial: Lazifing your kernel matrices\n",
    "\n",
    "Lazifying your tensor gives you Lazytensor, a gpytorch (g stands for Gaussian Processes) construct that makes your life much easier. What does it do?\n",
    "\n",
    "- Computing inverses and log determinant of a n by n matrix is known to be computationally heavy $O(n^3)$. However, there are computational tricks that speed up these computation, such as conjugate gradient for Matrix-vector product.\n",
    "- For more, please read https://docs.gpytorch.ai/en/v1.1.1/lazy.html\n",
    "\n",
    "There are three particular methods we will be using with the lazytensor construct:\n",
    "\n",
    "1. The log determinant function:\n",
    "    - `.logdet()`\n",
    "    \n",
    "2. The inverse matrix multiplication function:\n",
    "    - `.inv_matmul()`\n",
    "    \n",
    "3. The add diag function:\n",
    "    - `.add_diag()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with RBF Kernel\n",
    "\n",
    "RBF = RBFkernel()\n",
    "rbf = RBF(train_x)\n",
    "\n",
    "# Compute log determinant\n",
    "\n",
    "print(\"Log determinant:\", rbf.logdet())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Compute matrix inverse product (K+lambda I)^{-1} y\n",
    "\n",
    "print(rbf.add_diag(torch.tensor(1e-3).float()).inv_matmul(train_y)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "Now implement your own DeepRBFKernel class please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRBFkernel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, h_layer1, h_layer2, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Fill in something here        \n",
    "        \n",
    "        \n",
    "    def forward(self, x1, x2=None):\n",
    "        \n",
    "        # Fill in something here\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimise the Deep kernel parameters against a loss function\n",
    "\n",
    "To optimise for Deep kernel parameters, we will use the following function:\n",
    "\n",
    "\\begin{align}\n",
    "L = -y^\\top (K+\\lambda I)^{-1} y - \\log(|K+\\lambda I|)\n",
    "\\end{align}\n",
    "\n",
    "The first part of this loss computes the model fit while the second part acts as a complexity term.\n",
    "\n",
    "Can you code up a torch nn module class that returns $K + \\lambda I$ everytime you query it?\n",
    "\n",
    "After that, write up a function that takes in $K+\\lambda I$ and $y$ as input and output loss $L$. This will be our optimisation objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kernel_model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, h_layer1, h_layer2, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Fill in something here\n",
    "\n",
    "    def forward(self, x1):\n",
    "        \n",
    "        # Fill in something here\n",
    "        \n",
    "        return None\n",
    "    \n",
    "\n",
    "# Write up your own loss function:\n",
    "\n",
    "def nll(km, y):\n",
    "    \"\"\"\n",
    "    This is the negative loglikelihood score\n",
    "    \n",
    "    km must be a lazy tensor\n",
    "    and y is a torch tensor\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally\n",
    "\n",
    "Recall always in pytorch we work with the following things:\n",
    "\n",
    "1. a model that contains the parameters we want to learn\n",
    "\n",
    "2. an optimisation function that we want to optimise \n",
    "\n",
    "3. an optimiser such as SGD, Adam etc.\n",
    "\n",
    "Now we have everything ready, you can code up your optimisation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the deep kernel object\n",
    "\n",
    "input_dim = train_x.shape[1]\n",
    "h_layer1, h_layer2 = [30, 50]\n",
    "output_dim = 100\n",
    "\n",
    "KM = kernel_model(input_dim, h_layer1, h_layer2, output_dim)\n",
    "\n",
    "# Set up optimiser\n",
    "optim = torch.optim.Adam(KM.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up epoch\n",
    "epoch = 1000\n",
    "\n",
    "for i in range(epoch):\n",
    "\n",
    "    # Zero gradients from previous iteration\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    # Output from model\n",
    "    output = KM(train_x)\n",
    "    \n",
    "    # Calc loss and backprop gradients\n",
    "    loss = nll(output, train_y.reshape(-1, 1))\n",
    "    loss.backward()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f' % (\n",
    "            i + 1, training_iter, loss.item()\n",
    "        ))    \n",
    "        \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Given all parameters trained, how to predict targets given test points?\n",
    "\n",
    "Use the following:\n",
    "\n",
    "\\begin{align}\n",
    "f(x^*) = k(x^*, \\bf{x})(K_{\\bf{x}\\bf{x}} + \\lambda I)^{-1}y\n",
    "\\end{align}\n",
    "\n",
    "where $k$ is your deep kernel and lambda is the regularisation term and $k(x^*, \\bf{x})$ is the vector of evaluations $[k(x^*, x_1), ..., k(x^*, x_n)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = KM.deeprbf(test_x, train_x).evaluate() @ KM(train_x).inv_matmul(train_y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred.detach().numpy(), test_y.numpy(), \"x\")\n",
    "plt.plot(test_y, test_y, \"-\")\n",
    "plt.xlabel(\"truth\")\n",
    "plt.ylabel(\"prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for RBF, what do you expect?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
